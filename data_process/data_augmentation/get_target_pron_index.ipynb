{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ViWyCopwYOO8","outputId":"3f685532-6a88-4056-c6d5-7e4555fa61e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[52, 38, 90, 59, 84, 58, 58, 70, 78, 60, 66, 51, 67, 54, 48, 70, 59, 70, 50, 60, 65, 65, 69, 78, 51, 57, 69, 53, 67, 58, 33, 66, 71, 97, 63, 69, 57, 70, 65, 84, 64, 83, 67, 55, 53, 72, 61, 44, 76, 62, 101, 53, 60, 64, 82, 55, 67, 53, 61, 59, 53, 90, 75, 74, 65, 42, 74, 52, 69, 54, 56, 37, 57, 65, 81, 64, 71, 50, 50, 67, 58, 78, 58, 59, 57, 69, 52, 94, 63, 52, 70, 49, 50, 45, 48, 58, 56, 77, 80, 99, 59, 55, 57, 59, 61, 63, 51, 61, 81, 59, 57, 64, 71, 61, 67, 48, 24, 56, 66, 84, 67, 65, 31, 51, 67, 76, 56, 66, 60, 78, 75, 70, 154, 72, 73, 58, 66, 50, 54, 52, 58, 60, 67, 101, 57, 103, 25, 50, 55, 63, 80, 56, 60, 104, 30, 52, 59, 101, 66, 55, 57, 61, 60, 88, 89, 61, 55, 79, 69, 113, 88, 93, 51, 58, 56, 75, 63, 59, 79, 63, 63, 106, 85, 71, 119, 66, 49, 74, 55, 50, 92, 64, 71, 53, 57, 53, 91, 56, 52, 66, 53, 70, 51, 82, 61, 72, 106, 73, 58, 52, 58, 73, 91, 69, 39, 56, 68, 54, 69, 72, 79, 77, 66, 75, 61, 56, 69, 62, 58, 53, 47, 27, 51, 61, 56, 61, 85, 50, 84, 53, 52, 101, 57, 51, 61, 64, 72, 118, 64, 66, 51, 78, 76, 86, 64, 76, 63, 67, 60, 57, 73, 83, 80, 73, 65, 73, 36, 71, 64, 69, 57, 74, 44, 56, 63, 60, 76, 59, 98, 51, 62, 65, 66, 71, 61, 80, 55, 110, 58, 79, 63, 52, 25, 199, 111, 51, 60, 54, 61, 59, 92, 57, 63, 68, 81, 78, 55, 66, 24, 62, 72, 54, 66, 57, 52, 80, 46, 50, 63, 71, 76, 48, 74, 52, 56, 60, 50, 18, 53, 54, 77, 62, 69, 59, 72, 54, 67, 53, 66, 50, 57, 77, 64, 29, 54, 50, 65, 130, 54, 61, 61, 65, 63, 58, 52, 1, 51, 66, 54, 55, 100, 66, 58, 72, 53, 77, 65, 71, 82, 63, 84, 61, 84, 52, 62, 58, 70, 57, 63, 62, 80, 75, 67, 76, 58, 61, 56, 64, 64, 74, 71, 60, 84, 54, 73, 70, 67, 58, 59, 88, 72, 53, 86, 68, 91, 55, 69, 61, 80, 73, 68, 45, 64, 60, 31, 67, 62, 71, 74, 81, 72, 64, 56, 64, 63, 54, 50, 72, 77, 33, 74, 67, 84, 62, 57, 71, 55, 88, 135, 56, 90, 1, 69, 132, 60, 68, 50, 69, 43, 66, 78, 55, 75, 53]\n"]}],"source":["import pandas as pd\n","import ast\n","import spacy\n","\n","# IMPORTANT: put path in to data\n","csv_file_path =  ... # 'PATH'\n","\n","df = pd.read_csv(csv_file_path)\n","\n","\n","# get word index of target pronoun\n","# IMPORTANT: use SPACY for tokenization!! whenever you tokenize!!!\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def find_word_offset(text, char_offset):\n","    # tokenizes text\n","    doc = nlp(text)\n","    word_index = 0\n","    char_count = 0\n","\n","    for token in doc:\n","        # for every token, add to char_count (add length of the token's text + whitespace)\n","        char_count += len(token.text_with_ws)\n","        if char_count > char_offset:\n","            break\n","        else:\n","            if not token.is_space:\n","                # calc word offset\n","                word_index += 1\n","\n","    return word_index\n","\n","\n","all_target_indexes = []\n","\n","for index, row in df.iterrows():\n","    text= df.loc[index, 'Text']\n","    char_offset = df.loc[index, 'Pronoun-offset']\n","\n","    # commented out code gets POS_tag\n","    \"\"\"\n","    # need to convert pos_tags value into a list (not a string!)\n","    tags_str = df.loc[index, 'POS_Tags']\n","    tag = ast.literal_eval(tags_str)\n","    \"\"\"\n","\n","    word_index = find_word_offset(text, char_offset)\n","\n","    all_target_indexes.append(word_index)\n","\n","    \"\"\"\"\n","    if word_offset >= len(tag):\n","        pron_pos_tag = None\n","    else:\n","        pron_pos_tag = tag[word_offset]\n","    \"\"\"\n","\n","# target index for all rows in dataset!\n","# can make into dataframe if you want\n","print(all_target_indexes)\n","\n",""]}],"metadata":{"kernelspec":{"display_name":"ML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}